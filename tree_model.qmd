---
title: "Decision Tree Model"
format:
  html:
    toc: true
    code-fold: true
---


We trained a decision tree model to predict heart disease using the `rpart` engine via the `tidymodels` framework. This model is inherently interpretable and serves as a baseline for comparison.


## Model Setup and Training

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
theme_set(theme_minimal())

# Load unified setup and data
source("setup.R")
```


```{r}
# Preprocessing Recipe
heart_recipe <- recipe(HeartDisease ~ ., data = heart_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

# Specify Decision Tree Model
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

heart_wf <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(heart_recipe)

# Train the Model
tree_fit <- fit(heart_wf, data = heart_train)

```


## Confusion Matrix

```{r}
tree_preds <- predict(tree_fit, heart_test, type = "prob") %>%
  bind_cols(predict(tree_fit, heart_test)) %>%
  bind_cols(heart_test)

tree_preds %>%
  conf_mat(truth = HeartDisease, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title = "Confusion Matrix - Decision Tree") +
  theme_minimal()
```

This confusion matrix shows that the decision tree model performs well on both classes, achieving balanced sensitivity and specificity. The equal number of false positives and false negatives (21 each) suggests that the model does not exhibit bias toward either class. With an overall accuracy of 86.4%, the decision tree serves as a strong baseline for comparison with more complex models.


## ROC Curve

```{r}
tree_preds %>%
  roc_curve(truth = HeartDisease, .pred_1, event_level = "second") %>%
  autoplot() +
  labs(title = "ROC Curve - Decision Tree") +
  theme_minimal()

```

The ROC curve for the decision tree model rises steeply toward the top-left corner, indicating strong discriminative ability between positive and negative classes. This shape suggests that the model achieves a high true positive rate while maintaining a low false positive rate across a range of thresholds.

The curve significantly deviates from the diagonal line (representing random guessing), confirming that the model performs much better than chance. This supports the decision tree as a reliable baseline model for predicting heart disease.


## Train vs. Test Comparison

```{r}
tree_preds_train <- predict(tree_fit, heart_train, type = "prob") %>%
  bind_cols(predict(tree_fit, heart_train)) %>%
  bind_cols(heart_train)

# Define metric set
metrics <- metric_set(accuracy, roc_auc)

# Apply to training predictions
metrics(tree_preds_train,
           truth = HeartDisease,
           estimate = .pred_class,
           .pred_1,
           event_level = "second")


```

On the training set, the decision tree achieved an accuracy of 89.96% and a ROC AUC of 0.93 after correcting the event level. These results indicate that the model not only predicts accurately, but also ranks predictions effectively.

Compared to the test set performance (accuracy 86.4%, AUC X.XX), the small drop suggests that the model generalizes reasonably well without severe overfitting. The training performance confirms that the decision tree has learned meaningful patterns from the data.


## Global Feature Importance

```{r}
library(rpart)

# Extract variable importance from underlying fitted model
rpart_fit <- extract_fit_parsnip(tree_fit)$fit

vip <- data.frame(
  Feature = names(rpart_fit$variable.importance),
  Importance = as.numeric(rpart_fit$variable.importance)
) %>%
  arrange(desc(Importance)) %>%
  slice_head(n = 5)


# Plot top 5 features
vip %>%
  ggplot(aes(x = fct_reorder(Feature, Importance), y = Importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 5 Feature Importances (Decision Tree)", x = NULL, y = "Importance")

```

The top 5 most important features for the decision tree include two Thalassemia categories (Thalassemia_X2 and Thalassemia_X3), followed by the number of major vessels, Oldpeak, and MaxHR. These features drive the majority of the splits in the tree, indicating their strong influence in predicting heart disease.


## tree plot

```{r}
library(rpart.plot)
rpart.plot(rpart_fit, type = 2, extra = 106)

```

The decision tree visualization shows that the top splitting variable is Thalassemia_X2, indicating it is the most important feature in predicting heart disease. The left subtree mostly leads to predictions of no heart disease (0), while the right subtree often predicts presence of heart disease (1) with high confidence. Other key decision nodes include NumMajorVessels, Oldpeak, ChestPainType, and RestingBP, suggesting these features also play a significant role in the classification. The tree structure confirms the modelâ€™s interpretability and highlights clinically relevant variables.


## Save for Comparison

```{r}
# Save top features for LIME comparison
tree_top_features <- vip$Feature
saveRDS(tree_top_features, file = "scripts/tree_top_features.rds")

# Save fitted model for LIME explanation
saveRDS(tree_fit, file = "scripts/tree_fit.rds")

```


## Model Summary

The decision tree model offers a strong balance between interpretability and predictive performance.

From the tree visualization, the top splitting feature is **Thalassemia_X2**, indicating it plays a crucial role in distinguishing patients with heart disease. Other important decision nodes include **Thalassemia_X3**, **NumMajorVessels**, **Oldpeak**, and **MaxHR**, as confirmed by both the tree structure and variable importance plot.

The confusion matrix shows balanced predictions with **129 true negatives and 137 true positives**, and equal false positives and false negatives (21 each), reflecting well-calibrated sensitivity and specificity.

The ROC curve demonstrates excellent discriminative power, with an AUC of **0.93 on the training set**, suggesting the model can effectively rank positive and negative cases. The high training accuracy (**89.96%**) supports its ability to capture meaningful patterns from the data without overfitting.

Overall, the decision tree provides a transparent and reasonably accurate baseline model. While interpretable, it may still benefit from more robust ensemble methods like random forests to improve generalization and reduce variance in complex datasets.
