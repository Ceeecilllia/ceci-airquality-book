
---
title: "Conclusion"
format: html
---

## Final Summary & Insights

This final section summarizes our findings by comparing model performance, interpretation consistency, and implications for medical trust.

---

### 1. Model Accuracy & AUC Overview

All models were evaluated on the same held-out test set using Accuracy and AUC.

```{r}
model_results <- tibble::tibble(
  Model = c("Decision Tree", "Random Forest", "SVM", "Neural Network", "Logistic Regression"),
  Accuracy = c(0.79, 0.85, 0.75, 0.76, 0.81),
  AUC = c(0.84, 0.89, 0.78, 0.79, 0.86)
)

knitr::kable(model_results, caption = "Test Accuracy and AUC for all models")
```

---

### 2. Global vs Local Feature Importance

We compare each model's global importance (e.g., `varImp`, `importance`) to its average LIME feature weights.

#### Decision Tree: Global vs LIME

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(forcats)

tree_global <- readRDS("scripts/tree_top_features.rds") %>% 
  tibble::tibble(Feature = .) %>% 
  mutate(Source = "Global")

lime_tree <- read_csv("data/lime_tree_explanations.csv") %>%
  group_by(feature_desc) %>%
  summarise(avg_weight = mean(feature_weight)) %>%
  arrange(desc(avg_weight)) %>%
  slice_head(n = 5) %>%
  rename(Feature = feature_desc) %>%
  mutate(Source = "LIME")

tree_compare <- bind_rows(tree_global, lime_tree)

ggplot(tree_compare, aes(x = fct_reorder(Feature, as.numeric(Source == "Global")), 
                         y = ifelse(Source == "Global", 1, avg_weight), 
                         fill = Source)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Decision Tree: Global vs LIME Feature Comparison", y = "Importance / Weight", x = NULL)
```

#### Random Forest: Global vs LIME

```{r}
rf_global <- readRDS("scripts/rf_top_features.rds") %>% 
  tibble::tibble(Feature = .) %>% 
  mutate(Source = "Global")

lime_rf <- read_csv("data/lime_rf_explanations.csv") %>%
  group_by(feature_desc) %>%
  summarise(avg_weight = mean(feature_weight)) %>%
  arrange(desc(avg_weight)) %>%
  slice_head(n = 5) %>%
  rename(Feature = feature_desc) %>%
  mutate(Source = "LIME")

rf_compare <- bind_rows(rf_global, lime_rf)

ggplot(rf_compare, aes(x = fct_reorder(Feature, as.numeric(Source == "Global")), 
                       y = ifelse(Source == "Global", 1, avg_weight), 
                       fill = Source)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Random Forest: Global vs LIME Feature Comparison", y = "Importance / Weight", x = NULL)
```

---

### 3. Interpretability & Medical Trust

| Model                | LIME Support | Stability | Matches Global? | Medical Trust |
|---------------------|--------------|-----------|------------------|----------------|
| Decision Tree        | ✅ Yes       | High      | Good             | ✅ Yes         |
| Random Forest        | ✅ Yes       | Moderate  | Moderate         | ✅ Partial     |
| SVM                  | ❌ No        | —         | —                | ❌ No          |
| Neural Network       | ❌ No        | —         | —                | ❌ No          |
| Logistic Regression  | ✅ via IML   | High      | Excellent        | ✅ Yes         |

---

### Conclusion

- Models like **Decision Tree** and **Logistic Regression** offer strong interpretability and align well with their LIME/IML explanations.
- **Random Forest** balances performance and explainability with moderate stability.
- **SVM** and **Neural Network** performed reasonably but lacked interpretable support from LIME in our pipeline.
- For medical settings, interpretable models with consistent explanations are recommended for building trust and transparency.

