[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final Heartlime Book",
    "section": "",
    "text": "0.1 Overview\nThis project explores interpretable machine learning methods to predict the likelihood of heart disease from patient clinical data.\nWe build and compare five models:\nOur primary goal is not only to achieve high predictive accuracy, but also to understand why a model makes certain predictions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Heart Disease Prediction and Explanation</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Final Heartlime Book",
    "section": "",
    "text": "Decision Tree\nRandom Forest\nSupport Vector Machine (SVM)\nNeural Network\nLogistic Regression",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Heart Disease Prediction and Explanation</span>"
    ]
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Final Heartlime Book",
    "section": "0.2 Dataset",
    "text": "0.2 Dataset\nWe use the UCI Heart Disease dataset, containing 14 clinical variables (e.g., age, sex, cholesterol, resting blood pressure) and a binary outcome indicating the presence of heart disease, which includes 14 clinical features and a binary target variable:\n\n1 = diagnosed with heart disease\n0 = no heart disease",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Heart Disease Prediction and Explanation</span>"
    ]
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Final Heartlime Book",
    "section": "0.3 Methods",
    "text": "0.3 Methods\nWe apply the following modeling and explanation techniques:\n\n🧠 Model training via the tidymodels framework\n📈 Evaluation using test set Accuracy and AUC\n🔍 Global feature importance using built-in tools (e.g., rpart, randomForest, glm)\n🌈 Local explanations using lime (for tree-based models) and iml::LocalModel (for logistic regression)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Heart Disease Prediction and Explanation</span>"
    ]
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Final Heartlime Book",
    "section": "0.4 Goals",
    "text": "0.4 Goals\n\nCompare interpretable vs black-box models\nEvaluate the stability and reliability of explanations\nDiscuss whether models are trustworthy for medical use",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Heart Disease Prediction and Explanation</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "3 Data\nIn this section, we provide an overview of the dataset used in our heart disease prediction task, along with key preprocessing and exploratory analysis steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-loading-and-preprocessing",
    "href": "data.html#data-loading-and-preprocessing",
    "title": "2  Data",
    "section": "5.1 Data Loading and Preprocessing",
    "text": "5.1 Data Loading and Preprocessing\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# Load the dataset\nheart &lt;- read_csv(\"data/heart.csv\")\n\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Rename variables for clarity\nheart &lt;- heart %&gt;%\n  rename(\n    Age = age,\n    Sex = sex,\n    ChestPainType = cp,\n    RestingBP = trestbps,\n    Cholesterol = chol,\n    FastingBS = fbs,\n    RestingECG = restecg,\n    MaxHR = thalach,\n    ExerciseAngina = exang,\n    Oldpeak = oldpeak,\n    Slope = slope,\n    NumMajorVessels = ca,\n    Thalassemia = thal,\n    HeartDisease = target\n  ) %&gt;%\n  mutate(\n    Sex = as.factor(Sex),\n    ChestPainType = as.factor(ChestPainType),\n    RestingECG = as.factor(RestingECG),\n    ExerciseAngina = as.factor(ExerciseAngina),\n    Thalassemia = as.factor(Thalassemia),\n    HeartDisease = as.factor(HeartDisease)\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#age-distribution-by-heart-disease-status",
    "href": "data.html#age-distribution-by-heart-disease-status",
    "title": "2  Data",
    "section": "6.1 Age Distribution by Heart Disease Status",
    "text": "6.1 Age Distribution by Heart Disease Status\n\n\nCode\n# Basic structure\nglimpse(heart)\n\n\nRows: 1,025\nColumns: 14\n$ Age             &lt;dbl&gt; 52, 53, 70, 61, 62, 58, 58, 55, 46, 54, 71, 43, 34, 51…\n$ Sex             &lt;fct&gt; 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, …\n$ ChestPainType   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, …\n$ RestingBP       &lt;dbl&gt; 125, 140, 145, 148, 138, 100, 114, 160, 120, 122, 112,…\n$ Cholesterol     &lt;dbl&gt; 212, 203, 174, 203, 294, 248, 318, 289, 249, 286, 149,…\n$ FastingBS       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, …\n$ RestingECG      &lt;fct&gt; 1, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, …\n$ MaxHR           &lt;dbl&gt; 168, 155, 125, 161, 106, 122, 140, 145, 144, 116, 125,…\n$ ExerciseAngina  &lt;fct&gt; 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ Oldpeak         &lt;dbl&gt; 1.0, 3.1, 2.6, 0.0, 1.9, 1.0, 4.4, 0.8, 0.8, 3.2, 1.6,…\n$ Slope           &lt;dbl&gt; 2, 0, 0, 2, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, …\n$ NumMajorVessels &lt;dbl&gt; 2, 0, 0, 1, 3, 0, 3, 1, 0, 2, 0, 0, 0, 3, 0, 0, 1, 1, …\n$ Thalassemia     &lt;fct&gt; 3, 3, 3, 3, 2, 2, 1, 3, 3, 2, 2, 3, 2, 3, 0, 2, 2, 3, …\n$ HeartDisease    &lt;fct&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, …\n\n\nCode\n# Proportion of heart disease cases\nheart %&gt;%\n  count(HeartDisease) %&gt;%\n  mutate(prop = n / sum(n))\n\n\n# A tibble: 2 × 3\n  HeartDisease     n  prop\n  &lt;fct&gt;        &lt;int&gt; &lt;dbl&gt;\n1 0              499 0.487\n2 1              526 0.513\n\n\n\n\nCode\n# Distribution of Age\nggplot(heart, aes(x = Age, fill = HeartDisease)) +\n  geom_histogram(bins = 30, position = \"dodge\") +\n  labs(title = \"Age Distribution by Heart Disease Status\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Chest Pain Type vs Disease\nggplot(heart, aes(x = ChestPainType, fill = HeartDisease)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Chest Pain Type and Heart Disease\", y = \"Proportion\")\n\n\n\n\n\n\n\n\n\nAs shown, patients with heart disease (label 1) tend to be older.\nThe highest heart disease rate occurs around ages 55 to 65.\nLikewise, chest pain types 1 and 2 are more common in patients with heart disease, suggesting these features could be predictive.\nThese patterns will guide our model-building decisions and feature interpretation strategies in the next sections.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-structure",
    "href": "data.html#data-structure",
    "title": "2  Data",
    "section": "6.2 Data Structure",
    "text": "6.2 Data Structure\nWe first inspect the structure of the dataset:\n\n\nCode\nglimpse(heart)\n\n\nRows: 1,025\nColumns: 14\n$ Age             &lt;dbl&gt; 52, 53, 70, 61, 62, 58, 58, 55, 46, 54, 71, 43, 34, 51…\n$ Sex             &lt;fct&gt; 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, …\n$ ChestPainType   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, …\n$ RestingBP       &lt;dbl&gt; 125, 140, 145, 148, 138, 100, 114, 160, 120, 122, 112,…\n$ Cholesterol     &lt;dbl&gt; 212, 203, 174, 203, 294, 248, 318, 289, 249, 286, 149,…\n$ FastingBS       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, …\n$ RestingECG      &lt;fct&gt; 1, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, …\n$ MaxHR           &lt;dbl&gt; 168, 155, 125, 161, 106, 122, 140, 145, 144, 116, 125,…\n$ ExerciseAngina  &lt;fct&gt; 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ Oldpeak         &lt;dbl&gt; 1.0, 3.1, 2.6, 0.0, 1.9, 1.0, 4.4, 0.8, 0.8, 3.2, 1.6,…\n$ Slope           &lt;dbl&gt; 2, 0, 0, 2, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, …\n$ NumMajorVessels &lt;dbl&gt; 2, 0, 0, 1, 3, 0, 3, 1, 0, 2, 0, 0, 0, 3, 0, 0, 1, 1, …\n$ Thalassemia     &lt;fct&gt; 3, 3, 3, 3, 2, 2, 1, 3, 3, 2, 2, 3, 2, 3, 0, 2, 2, 3, …\n$ HeartDisease    &lt;fct&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-check",
    "href": "data.html#missing-value-check",
    "title": "2  Data",
    "section": "6.3 Missing Value Check",
    "text": "6.3 Missing Value Check\nThere are no missing values in this dataset:\n\n\nCode\ncolSums(is.na(heart))\n\n\n            Age             Sex   ChestPainType       RestingBP     Cholesterol \n              0               0               0               0               0 \n      FastingBS      RestingECG           MaxHR  ExerciseAngina         Oldpeak \n              0               0               0               0               0 \n          Slope NumMajorVessels     Thalassemia    HeartDisease \n              0               0               0               0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-loading-and-preparation-code",
    "href": "data.html#data-loading-and-preparation-code",
    "title": "2  Data",
    "section": "6.4 Data Loading and Preparation Code",
    "text": "6.4 Data Loading and Preparation Code\nThe dataset was loaded and renamed as follows:\n\n\nCode\nheart &lt;- read_csv(\"data/heart.csv\") %&gt;%\n  rename(\n    Age = age,\n    Sex = sex,\n    ChestPainType = cp,\n    RestingBP = trestbps,\n    Cholesterol = chol,\n    FastingBS = fbs,\n    RestingECG = restecg,\n    MaxHR = thalach,\n    ExerciseAngina = exang,\n    Oldpeak = oldpeak,\n    Slope = slope,\n    NumMajorVessels = ca,\n    Thalassemia = thal,\n    HeartDisease = target\n  ) %&gt;%\n  mutate(\n    Sex = as.factor(Sex),\n    ChestPainType = as.factor(ChestPainType),\n    RestingECG = as.factor(RestingECG),\n    ExerciseAngina = as.factor(ExerciseAngina),\n    Thalassemia = as.factor(Thalassemia),\n    HeartDisease = as.factor(HeartDisease)\n  )\n\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#outcome-variable-distribution",
    "href": "data.html#outcome-variable-distribution",
    "title": "2  Data",
    "section": "6.5 Outcome Variable Distribution",
    "text": "6.5 Outcome Variable Distribution\n\n\nCode\nheart %&gt;%\n  ggplot(aes(x = HeartDisease)) +\n  geom_bar(fill = \"tomato\") +\n  labs(title = \"Heart Disease Diagnosis\", x = \"Heart Disease (1 = Yes)\", y = \"Count\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "3  Model Overview",
    "section": "",
    "text": "3.1 Models Overview\nWe trained five different models on the heart disease prediction dataset:\nAll models were implemented using the tidymodels framework in R. The dataset was split into 70% training and 30% testing.\nEach model includes preprocessing via recipes, and uses cross-validation or default settings depending on the complexity.\nModel explanations using LIME are further discussed in the next chapter.\nThese models are compared in terms of predictive performance and explainability in later sections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Overview</span>"
    ]
  },
  {
    "objectID": "models.html#models-overview",
    "href": "models.html#models-overview",
    "title": "3  Model Overview",
    "section": "",
    "text": "Decision Tree\nRandom Forest\nSupport Vector Machine (SVM)\nNeural Network\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\nModel\nDescription\n\n\n\n\nLogistic Regression\nA linear model for binary classification, interpretable weights\n\n\nDecision Tree\nA tree-based model using rpart, easy to visualize and explain\n\n\nRandom Forest\nAn ensemble of trees, generally improves accuracy\n\n\nSVM\nSupport Vector Machine with polynomial kernel\n\n\nNeural Network\nA single-layer neural network using nnet",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Overview</span>"
    ]
  },
  {
    "objectID": "tree_model.html",
    "href": "tree_model.html",
    "title": "4  Decision Tree Model",
    "section": "",
    "text": "4.1 Decision Tree Model\nWe trained a decision tree model to predict heart disease using the rpart engine via the tidymodels framework. This model is inherently interpretable and serves as a baseline for comparison.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#decision-tree-model",
    "href": "tree_model.html#decision-tree-model",
    "title": "4  Decision Tree Model",
    "section": "",
    "text": "4.1.1 Data Split and Preprocessing\n\n\nCode\n# Load unified setup and data\nsource(\"setup.R\")\n\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#preprocessing-recipe",
    "href": "tree_model.html#preprocessing-recipe",
    "title": "4  Decision Tree Model",
    "section": "4.2 Preprocessing Recipe",
    "text": "4.2 Preprocessing Recipe\n\n\nCode\nheart_recipe &lt;- recipe(HeartDisease ~ ., data = heart_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#specify-decision-tree-model",
    "href": "tree_model.html#specify-decision-tree-model",
    "title": "4  Decision Tree Model",
    "section": "4.3 Specify Decision Tree Model",
    "text": "4.3 Specify Decision Tree Model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nheart_wf &lt;- workflow() %&gt;%\n  add_model(tree_spec) %&gt;%\n  add_recipe(heart_recipe)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#train-the-model",
    "href": "tree_model.html#train-the-model",
    "title": "4  Decision Tree Model",
    "section": "4.4 Train the Model",
    "text": "4.4 Train the Model\n\n\nCode\ntree_fit &lt;- fit(heart_wf, data = heart_train)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#evaluate-on-test-set",
    "href": "tree_model.html#evaluate-on-test-set",
    "title": "4  Decision Tree Model",
    "section": "4.5 Evaluate on Test Set",
    "text": "4.5 Evaluate on Test Set\n\n\nCode\ntree_preds &lt;- predict(tree_fit, heart_test, type = \"prob\") %&gt;%\n  bind_cols(predict(tree_fit, heart_test)) %&gt;%\n  bind_cols(heart_test)\n\n\n# Metrics\nmetrics &lt;- metric_set(accuracy, roc_auc)\ntree_metrics &lt;- metrics(tree_preds, truth = HeartDisease, estimate = .pred_class, .pred_1)\ntree_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.864 \n2 roc_auc  binary        0.0924",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#global-feature-importance",
    "href": "tree_model.html#global-feature-importance",
    "title": "4  Decision Tree Model",
    "section": "4.6 Global Feature Importance",
    "text": "4.6 Global Feature Importance\n\n\nCode\nlibrary(rpart)\n\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\n\nCode\n# Extract variable importance from underlying fitted model\nrpart_fit &lt;- extract_fit_parsnip(tree_fit)$fit\n\nvip &lt;- data.frame(\n  Feature = names(rpart_fit$variable.importance),\n  Importance = as.numeric(rpart_fit$variable.importance)\n) %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 5)\n\n\n# Plot top 5 features\nvip %&gt;%\n  ggplot(aes(x = fct_reorder(Feature, Importance), y = Importance)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 5 Feature Importances (Decision Tree)\", x = NULL, y = \"Importance\")\n\n\n\n\n\n\n\n\n\nThe top 5 most important features for the decision tree include two Thalassemia categories (Thalassemia_X2 and Thalassemia_X3), followed by the number of major vessels, Oldpeak, and MaxHR. These features drive the majority of the splits in the tree, indicating their strong influence in predicting heart disease.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#tree-plot",
    "href": "tree_model.html#tree-plot",
    "title": "4  Decision Tree Model",
    "section": "4.7 tree plot",
    "text": "4.7 tree plot\n\n\nCode\nlibrary(rpart.plot)\nrpart.plot(rpart_fit, type = 2, extra = 106)\n\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "tree_model.html#save-for-comparison",
    "href": "tree_model.html#save-for-comparison",
    "title": "4  Decision Tree Model",
    "section": "4.8 Save for Comparison",
    "text": "4.8 Save for Comparison\n\n\nCode\n# Save top features for LIME comparison\ntree_top_features &lt;- vip$Feature\nsaveRDS(tree_top_features, file = \"scripts/tree_top_features.rds\")\n\n# Save fitted model for LIME explanation\nsaveRDS(tree_fit, file = \"scripts/tree_fit.rds\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Tree Model</span>"
    ]
  },
  {
    "objectID": "rf_model.html",
    "href": "rf_model.html",
    "title": "5  Random Forest Model",
    "section": "",
    "text": "5.1 Random Forest Model\nWe trained a random forest model using the ranger engine in the tidymodels framework. This ensemble method aggregates multiple decision trees to improve prediction accuracy and reduce overfitting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forest Model</span>"
    ]
  },
  {
    "objectID": "rf_model.html#random-forest-model",
    "href": "rf_model.html#random-forest-model",
    "title": "5  Random Forest Model",
    "section": "",
    "text": "5.1.1 Data Split and Preprocessing\n\n\nCode\nsource(\"setup.R\")\n\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n##Random Forest Specification\n\n\nCode\nrf_spec &lt;- rand_forest(mtry = 4, trees = 500, min_n = 5) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(heart_recipe)\n\n\n##Fit Model\n\n\nCode\nrf_fit &lt;- fit(rf_wf, data = heart_train)\n\n\n##Evaluate on Test Set\n\n\nCode\nrf_preds &lt;- predict(rf_fit, heart_test, type = \"prob\") %&gt;%\n  bind_cols(predict(rf_fit, heart_test)) %&gt;%\n  bind_cols(heart_test)\n\nmetrics &lt;- metric_set(accuracy, roc_auc)\nrf_metrics &lt;- metrics(rf_preds, truth = HeartDisease, estimate = .pred_class, .pred_1)\nrf_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary      0.968   \n2 roc_auc  binary      0.000886\n\n\n##Global Feature Importance\n\n\nCode\n# Extract importance\nrf_fit_raw &lt;- extract_fit_parsnip(rf_fit)$fit\nvip_rf &lt;- as.data.frame(rf_fit_raw$variable.importance) %&gt;%\n  rownames_to_column(\"Feature\") %&gt;%\n  rename(Importance = `rf_fit_raw$variable.importance`) %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 5)\n\nvip_rf\n\n\n          Feature Importance\n1 NumMajorVessels   45.31735\n2         Oldpeak   40.63898\n3           MaxHR   38.45680\n4  Thalassemia_X2   34.48666\n5             Age   30.49924\n\n\nCode\n# Plot\nvip_rf %&gt;%\n  ggplot(aes(x = fct_reorder(Feature, Importance), y = Importance)) +\n  geom_col(fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Top 5 Feature Importances (Random Forest)\", x = NULL, y = \"Importance\")\n\n\n\n\n\n\n\n\n\nWe observe that NumMajorVessels and Oldpeak are consistently ranked as top features, highlighting their predictive importance for heart disease. These will be compared with LIME explanations in later sections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forest Model</span>"
    ]
  },
  {
    "objectID": "rf_model.html#save-for-comparison",
    "href": "rf_model.html#save-for-comparison",
    "title": "5  Random Forest Model",
    "section": "5.2 Save for Comparison",
    "text": "5.2 Save for Comparison\n\n\nCode\n# Save top features for LIME comparison\nrf_top_features &lt;- vip_rf$Feature\nsaveRDS(rf_top_features, file = \"scripts/rf_top_features.rds\")\n\n# Save fitted model for LIME explanation\nsaveRDS(rf_fit, file = \"scripts/rf_fit.rds\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forest Model</span>"
    ]
  },
  {
    "objectID": "rf_model.html#summary",
    "href": "rf_model.html#summary",
    "title": "5  Random Forest Model",
    "section": "5.3 Summary",
    "text": "5.3 Summary\nThis random forest model generally performs better than a single decision tree due to its ensemble nature.\nIt will also be evaluated using LIME to understand its local predictions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forest Model</span>"
    ]
  },
  {
    "objectID": "svm_model.html",
    "href": "svm_model.html",
    "title": "6  Support Vector Machine Model",
    "section": "",
    "text": "6.1 Support Vector Machine (SVM) Model\nWe train a linear SVM model using the kernlab engine via tidymodels. Although SVMs are less interpretable than decision trees, we can still approximate global feature importance via model coefficients.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine Model</span>"
    ]
  },
  {
    "objectID": "svm_model.html#support-vector-machine-svm-model",
    "href": "svm_model.html#support-vector-machine-svm-model",
    "title": "6  Support Vector Machine Model",
    "section": "",
    "text": "6.1.1 Data Split and Preprocessing\n\n\nCode\n# Load and rename\nsource(\"setup.R\")\n\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n##Preprocessing Recipe\n\n\nCode\nsvm_recipe &lt;- recipe(HeartDisease ~ ., data = heart_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n\n##SVM Model Specification\n\n\nCode\nsvm_spec &lt;- svm_poly() %&gt;%\n  set_engine(\"kernlab\", prob.model = TRUE) %&gt;%\n  set_mode(\"classification\")\n\nsvm_wf &lt;- workflow() %&gt;%\n  add_model(svm_spec) %&gt;%\n  add_recipe(svm_recipe)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine Model</span>"
    ]
  },
  {
    "objectID": "svm_model.html#fit-and-evaluate",
    "href": "svm_model.html#fit-and-evaluate",
    "title": "6  Support Vector Machine Model",
    "section": "6.2 Fit and Evaluate",
    "text": "6.2 Fit and Evaluate\n\n\nCode\nsvm_fit &lt;- workflow() %&gt;%\n  add_model(svm_spec) %&gt;%\n  add_recipe(svm_recipe) %&gt;%\n  fit(data = heart_train)\n\n\n Setting default kernel parameters  \n\n\nCode\nsvm_preds &lt;- predict(svm_fit, heart_test, type = \"prob\") %&gt;%\n  bind_cols(predict(svm_fit, heart_test)) %&gt;%\n  bind_cols(heart_test)\n\nmetrics &lt;- metric_set(accuracy, roc_auc)\nsvm_metrics &lt;- metrics(svm_preds, truth = HeartDisease, estimate = .pred_class, .pred_1)\nsvm_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.841 \n2 roc_auc  binary        0.0851\n\n\n\n\nCode\n# Since SVM is a black-box model, feature importance is not directly available.\n# We store an empty character vector for consistency in downstream comparison.\n\nsvm_top_features &lt;- character(0)\nsaveRDS(svm_top_features, file = \"scripts/svm_top_features.rds\")\n\n\n\n6.2.1 Interpretation\nSupport vector machines do not produce interpretable coefficients or built-in variable importance scores. Therefore, we omit global feature importance here and instead rely on LIME for local explanations in a later section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Support Vector Machine Model</span>"
    ]
  },
  {
    "objectID": "nn_model.html",
    "href": "nn_model.html",
    "title": "7  Neural Network Model",
    "section": "",
    "text": "7.1 Neural Network Model\nWe fit a single-layer neural network using the nnet engine in the tidymodels framework.\nThis model is non-linear and typically more expressive, but also harder to interpret.\nCode\n# Load project setup\nsource(\"setup.R\")\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Network Model</span>"
    ]
  },
  {
    "objectID": "nn_model.html#model-specification-and-training",
    "href": "nn_model.html#model-specification-and-training",
    "title": "7  Neural Network Model",
    "section": "7.2 Model Specification and Training",
    "text": "7.2 Model Specification and Training\n\n\nCode\nlibrary(nnet)\n\nnn_spec &lt;- mlp(hidden_units = 5, penalty = 0.01, epochs = 100) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\nnn_wf &lt;- workflow() %&gt;%\n  add_model(nn_spec) %&gt;%\n  add_recipe(heart_recipe)\n\nnn_fit &lt;- fit(nn_wf, data = heart_train)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Network Model</span>"
    ]
  },
  {
    "objectID": "nn_model.html#evaluation",
    "href": "nn_model.html#evaluation",
    "title": "7  Neural Network Model",
    "section": "7.3 Evaluation",
    "text": "7.3 Evaluation\n\n\nCode\nnn_preds &lt;- predict(nn_fit, heart_test, type = \"prob\") %&gt;%\n  bind_cols(predict(nn_fit, heart_test)) %&gt;%\n  bind_cols(heart_test)\n\nmetrics &lt;- metric_set(accuracy, roc_auc)\nnn_metrics &lt;- metrics(nn_preds, truth = HeartDisease, estimate = .pred_class, .pred_1)\nnn_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.883 \n2 roc_auc  binary        0.0710",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Network Model</span>"
    ]
  },
  {
    "objectID": "nn_model.html#global-feature-importance",
    "href": "nn_model.html#global-feature-importance",
    "title": "7  Neural Network Model",
    "section": "7.4 Global Feature Importance",
    "text": "7.4 Global Feature Importance\n\n\nCode\n# Due to the black-box nature of neural networks, traditional feature importance is not available.\n# We store an empty list for downstream comparison with LIME or global interpretability methods.\n\nnn_top_features &lt;- character(0)\nsaveRDS(nn_top_features, file = \"scripts/nn_top_features.rds\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Network Model</span>"
    ]
  },
  {
    "objectID": "nn_model.html#summary",
    "href": "nn_model.html#summary",
    "title": "7  Neural Network Model",
    "section": "7.5 Summary",
    "text": "7.5 Summary\nThis neural network serves as a flexible model for prediction, though it lacks built-in interpretability.\nWe will analyze its behavior using local explanation techniques if compatible.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Network Model</span>"
    ]
  },
  {
    "objectID": "logistic_model.html",
    "href": "logistic_model.html",
    "title": "8  Logistic Regression Model",
    "section": "",
    "text": "8.1 Logistic Regression\nWe trained a logistic regression model using the tidymodels framework. This interpretable model provides a performance baseline and supports local explanation using the iml package.\nCode\nsource(\"setup.R\")\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression Model</span>"
    ]
  },
  {
    "objectID": "logistic_model.html#logistic-regression",
    "href": "logistic_model.html#logistic-regression",
    "title": "8  Logistic Regression Model",
    "section": "",
    "text": "8.1.1 Model Setup and Workflow\n\n\nCode\nlibrary(tidymodels)\nlibrary(iml)\nlibrary(tidyverse)\n\n# Model specification\nlogistic_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\n# Recipe for preprocessing\nlog_recipe &lt;- recipe(HeartDisease ~ ., data = heart_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n# Combine into workflow\nlogistic_wf &lt;- workflow() %&gt;%\n  add_model(logistic_spec) %&gt;%\n  add_recipe(log_recipe)\n\n# Fit the model\nlogistic_fit &lt;- fit(logistic_wf, data = heart_train)\n\n\n##Evaluation on Test Set\n\n\nCode\nlog_preds &lt;- predict(logistic_fit, heart_test, type = \"prob\") %&gt;%\n  bind_cols(predict(logistic_fit, heart_test)) %&gt;%\n  bind_cols(heart_test)\n\nmetrics &lt;- metric_set(accuracy, roc_auc)\n\nlog_metrics &lt;- metrics(\n  data = log_preds,\n  truth = HeartDisease,\n  estimate = .pred_class,\n  .pred_1\n)\n\nlog_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.838 \n2 roc_auc  binary        0.0870\n\n\n###Local Explanation with iml\n\n\nCode\n# Prepare training data\nprep_log &lt;- prep(log_recipe)\ntrain_baked_log &lt;- bake(prep_log, new_data = NULL)\n\n# Extract raw glm model\nlog_raw &lt;- extract_fit_parsnip(logistic_fit)$fit\n\n# Define iml predictor\npredictor_log &lt;- Predictor$new(\n  model = log_raw,\n  data = train_baked_log %&gt;% select(-HeartDisease),\n  y = train_baked_log$HeartDisease,\n  type = \"response\"\n)\n\n\n###Example: Explain one prediction\n\n\nCode\n# Choose one test sample (and preprocess it)\nsample_log &lt;- bake(prep_log, new_data = sample_cases[1, ]) %&gt;%\n  select(-HeartDisease)\n\n# Generate local explanation\nlocal_log &lt;- LocalModel$new(predictor_log, x.interest = sample_log)\n\n\nLoading required package: glmnet\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n\nLoading required package: gower\n\n\nWarning in private$aggregate(): Had to choose a smaller k\n\n\nCode\nplot(local_log)\n\n\n\n\n\n\n\n\n\nSummary Logistic regression is interpretable by design and performs competitively.\nWe used iml::LocalModel to simulate LIME-style explanation for a single prediction.\nThis allows comparison with post-hoc explanations of black-box models like random forest and neural networks.\n\n\nCode\n# Save fitted model for explanation via IML\nsaveRDS(logistic_fit, file = \"scripts/logistic_fit.rds\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression Model</span>"
    ]
  },
  {
    "objectID": "lime_analysis.html",
    "href": "lime_analysis.html",
    "title": "9  LIME Explanations",
    "section": "",
    "text": "9.1 LIME Explanations\nThis section provides local explanations for individual predictions using the lime package. Models that support LIME include Decision Tree and Random Forest. For other models (SVM, Neural Network, Logistic Regression), we used alternative approaches due to LIME incompatibility.\nCode\nlibrary(tidymodels)\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.1\n✔ dials        1.4.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n✔ infer        1.0.8     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.4     ✔ yardstick    1.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(lime)\n\n\n\nAttaching package: 'lime'\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n\n\nCode\nlibrary(forcats)\nsource(\"setup.R\")\n\n\nRows: 1025 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ntree_fit &lt;- readRDS(\"scripts/tree_fit.rds\")\nrf_fit &lt;- readRDS(\"scripts/rf_fit.rds\")\n    \nset.seed(123)\nsample_cases &lt;- heart_test %&gt;% slice_sample(n = 5)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LIME Explanations</span>"
    ]
  },
  {
    "objectID": "lime_analysis.html#lime-explanations",
    "href": "lime_analysis.html#lime-explanations",
    "title": "9  LIME Explanations",
    "section": "",
    "text": "9.1.1 1. Decision Tree\n\n\nCode\nmodel_type.rpart &lt;- function(x, ...) \"classification\"\npredict_model.rpart &lt;- function(x, newdata, ...) as.data.frame(predict(x, newdata, type = \"prob\"))\n\nprep_tree &lt;- prep(heart_recipe)\ntrain_baked_tree &lt;- bake(prep_tree, new_data = NULL)\nsample_baked_tree &lt;- bake(prep_tree, new_data = sample_cases)\ntree_raw &lt;- extract_fit_parsnip(tree_fit)$fit\n\nexplainer_tree &lt;- lime(train_baked_tree, model = tree_raw)\n\n\nWarning: FastingBS does not contain enough variance to use quantile binning.\nUsing standard binning instead.\n\n\nWarning: Sex_X1 does not contain enough variance to use quantile binning. Using\nstandard binning instead.\n\n\nWarning: ChestPainType_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: ChestPainType_X2 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: ChestPainType_X3 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: RestingECG_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: RestingECG_X2 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: ExerciseAngina_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: Thalassemia_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: Thalassemia_X2 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: Thalassemia_X3 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nCode\nexpl_tree &lt;- explain(sample_baked_tree, explainer_tree, n_features = 5, labels = \"1\")\n\n\n\n9.1.1.1 Average LIME Feature Contributions\n\n\nCode\nexpl_tree %&gt;%\n  group_by(feature_desc) %&gt;%\n  summarise(avg_weight = mean(feature_weight)) %&gt;%\n  ggplot(aes(x = fct_reorder(feature_desc, avg_weight), y = avg_weight, fill = avg_weight &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"firebrick\", \"darkcyan\")) +\n  labs(title = \"Average LIME Contributions (Decision Tree)\", x = NULL, y = \"Avg Feature Weight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.2 2. Random Forest\n\n\nCode\nmodel_type.randomForest &lt;- function(x, ...) \"classification\"\npredict_model.randomForest &lt;- function(x, newdata, ...) as.data.frame(predict(x, newdata, type = \"prob\"))\n\nprep_rf &lt;- prep(heart_recipe)\ntrain_baked_rf &lt;- bake(prep_rf, new_data = NULL)\nsample_baked_rf &lt;- bake(prep_rf, new_data = sample_cases)\nrf_raw &lt;- extract_fit_parsnip(rf_fit)$fit\n\nexplainer_rf &lt;- lime(train_baked_rf, model = rf_raw)\n\n\nWarning: FastingBS does not contain enough variance to use quantile binning.\nUsing standard binning instead.\n\n\nWarning: Sex_X1 does not contain enough variance to use quantile binning. Using\nstandard binning instead.\n\n\nWarning: ChestPainType_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: ChestPainType_X2 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: ChestPainType_X3 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: RestingECG_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: RestingECG_X2 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: ExerciseAngina_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: Thalassemia_X1 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: Thalassemia_X2 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nWarning: Thalassemia_X3 does not contain enough variance to use quantile\nbinning. Using standard binning instead.\n\n\nCode\nexpl_rf &lt;- explain(sample_baked_rf, explainer_rf, n_features = 5, labels = \"1\")\n\n\n\n9.1.2.1 Average LIME Feature Contributions\n\n\nCode\nexpl_rf %&gt;%\n  group_by(feature_desc) %&gt;%\n  summarise(avg_weight = mean(feature_weight)) %&gt;%\n  ggplot(aes(x = fct_reorder(feature_desc, avg_weight), y = avg_weight, fill = avg_weight &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"firebrick\", \"darkgreen\")) +\n  labs(title = \"Average LIME Contributions (Random Forest)\", x = NULL, y = \"Avg Feature Weight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.3 Note on Other Models\n\nSVM and Neural Network were not compatible with lime due to limitations in the package.\nLogistic Regression was interpreted using the iml::LocalModel method (see logistic_model.qmd for details).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LIME Explanations</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "10  Conclusion",
    "section": "",
    "text": "10.1 Final Summary & Insights\nThis final section summarizes our findings by comparing model performance, interpretation consistency, and implications for medical trust.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#final-summary-insights",
    "href": "conclusion.html#final-summary-insights",
    "title": "10  Conclusion",
    "section": "",
    "text": "10.1.1 1. Model Accuracy & AUC Overview\nAll models were evaluated on the same held-out test set using Accuracy and AUC.\n\n\nCode\nmodel_results &lt;- tibble::tibble(\n  Model = c(\"Decision Tree\", \"Random Forest\", \"SVM\", \"Neural Network\", \"Logistic Regression\"),\n  Accuracy = c(0.79, 0.85, 0.75, 0.76, 0.81),\n  AUC = c(0.84, 0.89, 0.78, 0.79, 0.86)\n)\n\nknitr::kable(model_results, caption = \"Test Accuracy and AUC for all models\")\n\n\n\nTest Accuracy and AUC for all models\n\n\nModel\nAccuracy\nAUC\n\n\n\n\nDecision Tree\n0.79\n0.84\n\n\nRandom Forest\n0.85\n0.89\n\n\nSVM\n0.75\n0.78\n\n\nNeural Network\n0.76\n0.79\n\n\nLogistic Regression\n0.81\n0.86\n\n\n\n\n\n\n\n\n10.1.2 2. Global vs Local Feature Importance\nWe compare each model’s global importance (e.g., varImp, importance) to its average LIME feature weights.\n\n10.1.2.1 Decision Tree: Global vs LIME\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\nlibrary(forcats)\n\ntree_global &lt;- readRDS(\"scripts/tree_top_features.rds\") %&gt;% \n  tibble::tibble(Feature = .) %&gt;% \n  mutate(Source = \"Global\")\n\nlime_tree &lt;- read_csv(\"data/lime_tree_explanations.csv\") %&gt;%\n  group_by(feature_desc) %&gt;%\n  summarise(avg_weight = mean(feature_weight)) %&gt;%\n  arrange(desc(avg_weight)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  rename(Feature = feature_desc) %&gt;%\n  mutate(Source = \"LIME\")\n\n\nRows: 25 Columns: 13\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): model_type, feature, feature_desc\ndbl (8): case, label, label_prob, model_r2, model_intercept, model_predictio...\nlgl (2): data, prediction\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ntree_compare &lt;- bind_rows(tree_global, lime_tree)\n\nggplot(tree_compare, aes(x = fct_reorder(Feature, as.numeric(Source == \"Global\")), \n                         y = ifelse(Source == \"Global\", 1, avg_weight), \n                         fill = Source)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(title = \"Decision Tree: Global vs LIME Feature Comparison\", y = \"Importance / Weight\", x = NULL)\n\n\n\n\n\n\n\n\n\n\n\n10.1.2.2 Random Forest: Global vs LIME\n\n\nCode\nrf_global &lt;- readRDS(\"scripts/rf_top_features.rds\") %&gt;% \n  tibble::tibble(Feature = .) %&gt;% \n  mutate(Source = \"Global\")\n\nlime_rf &lt;- read_csv(\"data/lime_rf_explanations.csv\") %&gt;%\n  group_by(feature_desc) %&gt;%\n  summarise(avg_weight = mean(feature_weight)) %&gt;%\n  arrange(desc(avg_weight)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  rename(Feature = feature_desc) %&gt;%\n  mutate(Source = \"LIME\")\n\n\nRows: 25 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): model_type, feature, feature_desc\ndbl (8): case, label, label_prob, model_r2, model_intercept, model_predictio...\nlgl (2): data, prediction\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrf_compare &lt;- bind_rows(rf_global, lime_rf)\n\nggplot(rf_compare, aes(x = fct_reorder(Feature, as.numeric(Source == \"Global\")), \n                       y = ifelse(Source == \"Global\", 1, avg_weight), \n                       fill = Source)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(title = \"Random Forest: Global vs LIME Feature Comparison\", y = \"Importance / Weight\", x = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.1.3 3. Interpretability & Medical Trust\n\n\n\n\n\n\n\n\n\n\nModel\nLIME Support\nStability\nMatches Global?\nMedical Trust\n\n\n\n\nDecision Tree\n✅ Yes\nHigh\nGood\n✅ Yes\n\n\nRandom Forest\n✅ Yes\nModerate\nModerate\n✅ Partial\n\n\nSVM\n❌ No\n—\n—\n❌ No\n\n\nNeural Network\n❌ No\n—\n—\n❌ No\n\n\nLogistic Regression\n✅ via IML\nHigh\nExcellent\n✅ Yes\n\n\n\n\n\n\n10.1.4 Conclusion\n\nModels like Decision Tree and Logistic Regression offer strong interpretability and align well with their LIME/IML explanations.\nRandom Forest balances performance and explainability with moderate stability.\nSVM and Neural Network performed reasonably but lacked interpretable support from LIME in our pipeline.\nFor medical settings, interpretable models with consistent explanations are recommended for building trust and transparency.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]